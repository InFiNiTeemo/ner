#!/usr/bin/env python
# coding: utf-8

# ### Notebook for finetuning H2o-danube-1.8b-base Model using Pytorch Lightning.  
# 
# You can find more details about the model: 
# 
# **Research paper:** https://arxiv.org/abs/2401.16818
# 
# **Model Huggingface card:** https://huggingface.co/h2oai/h2o-danube-1.8b-base

# #### Inference Notebook: https://www.kaggle.com/code/nischaydnk/h2o-danube-1-8b-llm-submission
# 
# #### Settings to get 0.962+ CV:
# - Training Sequence Length - 1400
# - Downsample competition Data with samples having only 'O' labels with 0.75 ratio
# - Use MPware dataset shared here: https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/477989
# 

# # ðŸšš Imports

# In[1]:








# # ðŸ“Š Preprocessing

# In[8]:





# # Record

# In[25]:





# # ðŸ§  Model

# In[26]:





# In[27]:





# In[31]:



    


# In[32]:



    


